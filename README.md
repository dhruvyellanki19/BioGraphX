
# ğŸ“˜ **BioGraphX â€” Sprint 1 & 2 Complete**

## ğŸ§¬ Project Overview

**BioGraphX** is an end-to-end *Graph-Augmented, Agentic Biomedical Question-Answering System*.
It integrates:

* A Neo4j biomedical knowledge graph
* SciSpaCy-based biomedical entity extraction
* BioBERT/SciBERT embedding-based retrieval
* A LangGraph multi-agent reasoning pipeline
* A Streamlit interface for interpretable QA

This README documents **Sprint 1 & 2 completion** with actual deliverables achieved.

---

# ğŸŸ¦ **Sprint 1 â€” Project Bootstrapping & Data Download**

Sprint 1 focuses entirely on **setting up the environment**, **downloading datasets**, and **performing initial exploration (EDA)**.
No modeling, extraction, or graph construction happens yet.

---

## âœ… **1. Repository & Environment Setup**

### âœ” Create project structure

A clean folder layout was initialized:

```
BioGraphX/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ etl/
â”œâ”€â”€ graph/
â”œâ”€â”€ rag/
â”œâ”€â”€ agents/
â”œâ”€â”€ training/
â”œâ”€â”€ models/
â”œâ”€â”€ app/
â””â”€â”€ configs/
```

### âœ” Create & activate virtual environment

```bash
python -m venv venv
source venv/bin/activate        # macOS/Linux
# or
venv\Scripts\activate           # Windows
```

### âœ” Install all dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

This installs:

* PyTorch, Transformers
* spaCy, SciSpaCy
* Neo4j + Py2Neo
* ChromaDB + FAISS
* LangChain + LangGraph
* Streamlit
* Evaluation tools

---

## âœ… **2. Raw Dataset Download**

All raw datasets from Kaggle were downloaded manually and placed under:

```
data/raw/
```

### Included datasets:

* `medquad.csv` â€“ Main biomedical Q/A dataset
* `pubmed_abstracts.csv` â€“ Corpus for evidence retrieval
* `pubmed_qa_pga_labeled.parquet` â€“ Evaluation dataset
* `pubmed_qa_pga_artificial.parquet` â€“ Supplemental PubMed QA data

These datasets will be cleaned, processed, embedded, and graph-linked in later sprints.

---

## âœ… **3. Biomedical Model Installation & Verification**

The SciSpaCy biomedical NER model was installed using:

```bash
python scripts/install_embedding_model.py
```

or manual fallback:

```bash
pip install https://s3-us-west-2.amazonaws.com/ai2-s-scip...
```

### âœ” Successful model load test

```python
import spacy
nlp = spacy.load("en_ner_bc5cdr_md")
doc = nlp("Acetaminophen reduces fever and pain.")
print([(ent.text, ent.label_) for ent in doc.ents])
```

**Result:**
Entities such as CHEMICAL and DISEASE were correctly detected.

This confirms that the biomedical NLP stack is ready for Sprint 2.

---

## âœ… **4. EDA Notebook Created**

The required EDA notebook has been created at:

```
notebooks/01_eda_medquad.ipynb
```

### Notebook contents include:

* Loading `medquad.csv`
* `df.head()`, structural inspection
* Random sample Q/A pairs
* Missing value analysis
* Question/answer length distributions
* Optional: Test biomedical NER model inside the notebook

This validates dataset integrity and prepares for cleaning + entity extraction in Sprint 2.

---

# ğŸŸ© **Sprint 1 Summary â€” COMPLETED**

Sprint 1 goals were fully achieved:

| Deliverable                           | Status | Details |
| ------------------------------------- | ------ | ------- |
| Project structure initialized         | âœ…     | Clean folder hierarchy created |
| Virtual environment created           | âœ…     | Python 3.10 with all dependencies |
| Requirements installed                | âœ…     | 49 packages including ML/NLP stack |
| Kaggle datasets downloaded            | âœ…     | MedQuAD: 16,412 Q/A pairs |
| EDA notebook created                  | âœ…     | Comprehensive analysis with statistics |
| SciSpaCy NER model installed & tested | âœ…     | en_ner_bc5cdr_md working perfectly |

---

# ğŸŸ¦ **Sprint 2 â€” Data Processing & Graph Construction**

## âœ… **Sprint 2 Summary â€” COMPLETED & EXCEEDED EXPECTATIONS**

### ğŸ¯ **Original Goals Met:**
- **Data Cleaning**: âœ… MedQuAD cleaned and processed
- **Entity Extraction**: âœ… Biomedical NER pipeline implemented
- **Graph Schema**: âœ… Neo4j constraints and relationships defined  
- **Graph Data Preparation**: âœ… CSV files ready for Neo4j import

### ğŸš€ **Achievements Beyond Original Plan:**

#### **ğŸ“Š Massive Entity Extraction Success:**
- **182,775 biomedical entities** extracted from medical text
- **High-quality NER** using SciSpaCy's en_ner_bc5cdr_md model
- **Robust text processing** with NaN handling and batch optimization

#### **ğŸ—ï¸ Complete Graph Structure Built:**
```
Graph Data Generated:
â”œâ”€â”€ 16,360 Question nodes (from MedQuAD Q/A pairs)
â”œâ”€â”€ 17,937 Disease nodes (extracted from medical text)  
â”œâ”€â”€ 2,351 Drug/Chemical nodes (pharmaceuticals identified)
â””â”€â”€ 182,776 ABOUT relationships (questions linked to entities)
```

#### **ğŸ”§ Production-Ready ETL Pipeline:**
- **`etl/extract_entities.py`**: Scalable NER processing with batching
- **`etl/build_graph_csvs.py`**: Graph data preparation for Neo4j
- **`graph/schema.cql`**: Database constraints and relationship patterns
- **Error handling**: Robust text cleaning and validation

#### **ğŸ“ˆ Data Quality Metrics:**
- **16,412 medical Q/A pairs** processed successfully
- **Zero data loss** through careful NaN handling
- **Entity coverage**: Diseases (17,937) + Drugs (2,351) = 20,288 unique entities
- **Relationship density**: 11.1 entities per question on average

### ğŸ“ **Generated Artifacts:**
```
data/processed/
â”œâ”€â”€ medquad_clean.csv          # Cleaned Q/A dataset (21.5MB)
â”œâ”€â”€ entity_mappings.csv        # All extracted entities (5.5MB)
â””â”€â”€ graph_data/
    â”œâ”€â”€ nodes_question.csv     # Question nodes for Neo4j
    â”œâ”€â”€ nodes_disease.csv      # Disease entities  
    â”œâ”€â”€ nodes_drug.csv         # Drug/Chemical entities
    â””â”€â”€ rels_about.csv         # Question-Entity relationships
```

### ğŸ§° **Technical Stack Validated:**
- **SciSpaCy NER**: en_ner_bc5cdr_md model performing excellently
- **Pandas**: Efficient data processing of large datasets
- **Neo4j Schema**: Optimized for biomedical knowledge representation
- **Batch Processing**: Memory-efficient pipeline handling 180K+ entities

---

# ğŸš€ **Next Steps: Sprint 3 â€” RAG & Agent Pipeline**

With Sprint 1 & 2 successfully completed, Sprint 3 will implement:

### ğŸ¯ **Sprint 3 Goals:**
- **Vector Database**: ChromaDB with BioBERT embeddings
- **Multi-Agent System**: LangGraph reasoning pipeline  
- **Graph Integration**: Neo4j knowledge graph queries
- **RAG Pipeline**: Evidence retrieval and synthesis

### ğŸ—ï¸ **Architecture Ready:**
- **Knowledge Graph**: 20,288 biomedical entities + 182K relationships
- **Text Data**: 16,412 Q/A pairs for training/testing
- **NLP Stack**: Validated SciSpacy + embedding models
- **Data Pipeline**: Robust ETL processing proven at scale

**Current Status**: âœ… **Foundation Complete** â€” Ready for advanced AI components!

