
# ðŸ“˜ **BioGraphX â€” Sprint 1 Documentation**

## ðŸ§¬ Project Overview

**BioGraphX** is an end-to-end *Graph-Augmented, Agentic Biomedical Question-Answering System*.
It integrates:

* A Neo4j biomedical knowledge graph
* SciSpaCy-based biomedical entity extraction
* BioBERT/SciBERT embedding-based retrieval
* A LangGraph multi-agent reasoning pipeline
* A Streamlit interface for interpretable QA

This README documents **Sprint 1 deliverables** as defined in the project plan.

---

# ðŸŸ¦ **Sprint 1 â€” Project Bootstrapping & Data Download**

Sprint 1 focuses entirely on **setting up the environment**, **downloading datasets**, and **performing initial exploration (EDA)**.
No modeling, extraction, or graph construction happens yet.

---

## âœ… **1. Repository & Environment Setup**

### âœ” Create project structure

A clean folder layout was initialized:

```
BioGraphX/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ etl/
â”œâ”€â”€ graph/
â”œâ”€â”€ rag/
â”œâ”€â”€ agents/
â”œâ”€â”€ training/
â”œâ”€â”€ models/
â”œâ”€â”€ app/
â””â”€â”€ configs/
```

### âœ” Create & activate virtual environment

```bash
python -m venv venv
source venv/bin/activate        # macOS/Linux
# or
venv\Scripts\activate           # Windows
```

### âœ” Install all dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

This installs:

* PyTorch, Transformers
* spaCy, SciSpaCy
* Neo4j + Py2Neo
* ChromaDB + FAISS
* LangChain + LangGraph
* Streamlit
* Evaluation tools

---

## âœ… **2. Raw Dataset Download**

All raw datasets from Kaggle were downloaded manually and placed under:

```
data/raw/
```

### Included datasets:

* `medquad.csv` â€“ Main biomedical Q/A dataset
* `pubmed_abstracts.csv` â€“ Corpus for evidence retrieval
* `pubmed_qa_pga_labeled.parquet` â€“ Evaluation dataset
* `pubmed_qa_pga_artificial.parquet` â€“ Supplemental PubMed QA data

These datasets will be cleaned, processed, embedded, and graph-linked in later sprints.

---

## âœ… **3. Biomedical Model Installation & Verification**

The SciSpaCy biomedical NER model was installed using:

```bash
python scripts/install_embedding_model.py
```

or manual fallback:

```bash
pip install https://s3-us-west-2.amazonaws.com/ai2-s-scip...
```

### âœ” Successful model load test

```python
import spacy
nlp = spacy.load("en_ner_bc5cdr_md")
doc = nlp("Acetaminophen reduces fever and pain.")
print([(ent.text, ent.label_) for ent in doc.ents])
```

**Result:**
Entities such as CHEMICAL and DISEASE were correctly detected.

This confirms that the biomedical NLP stack is ready for Sprint 2.

---

## âœ… **4. EDA Notebook Created**

The required EDA notebook has been created at:

```
notebooks/01_eda_medquad.ipynb
```

### Notebook contents include:

* Loading `medquad.csv`
* `df.head()`, structural inspection
* Random sample Q/A pairs
* Missing value analysis
* Question/answer length distributions
* Optional: Test biomedical NER model inside the notebook

This validates dataset integrity and prepares for cleaning + entity extraction in Sprint 2.

---

# ðŸŸ© **Sprint 1 Summary**

Sprint 1 goals were fully achieved:

| Deliverable                           | Status |
| ------------------------------------- | ------ |
| Project structure initialized         | âœ”      |
| Virtual environment created           | âœ”      |
| Requirements installed                | âœ”      |
| Kaggle datasets downloaded            | âœ”      |
| EDA notebook created                  | âœ”      |
| SciSpaCy NER model installed & tested | âœ”      |

---

# ðŸš€ **Next Step: Sprint 2**

Sprint 2 will include:

* Data cleaning
* Biomedical entity extraction
* Neo4j schema definition
* Preparing graph node/edge CSVs

